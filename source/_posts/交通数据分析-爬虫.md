---
title: 交通数据分析-爬虫
mathjax: true
date: 2020-10-10 15:38:17
tags:
- python
- 爬虫
categories: 校内课程
---

《交通数据分析》课程中关于爬虫部分的内容，简单记录。

<!-- more -->

## HTTP与Web

HTTP是client与server之间的通讯协议，从server获取资源。

client发送URL（统一资源定位符）

- URL: Uniform Resource Locator
- URI: Uniform Resource Identifier
- URI可以是ftp，http，tel
  - URL是URI的子集

**URI绝对格式**
协议名称：登录信息@服务器地址:端口号（门）/带层次的文件路径？查询字符串#片段标识符（网页中相应片段）

## 网页数据采集

流程：
1. 发现**网址规律**，构建URL
2. 构建headers，cookies参数
3. 对目标网址发送请求（*伪装为浏览器访问*）
4. 解析返回的响应数据
5. 保存数据，重复3-5步骤

https://shanghai.8684.cn/ 交通数据网站

### 网站爬取

使用request包对内容爬取

响应码：200成功，4XX客户端错误，5XX服务器错误

```py
if __name__ == '__main__':
    url = 'https://shanghai.8684.cn/'
    r = requests.get(url)
    print(r.status_code)

```

**注意项目：**

- Encoding：utf-8
- HTML Response
- r.text是相应内容

**为了伪装成浏览器，加上headers，params等**

```py
if __name__ == '__main__':
    url = 'https://shanghai.8684.cn/so.php'
    parameters = {'q':'同济大学','q1':'东方明珠','k':'p2p'}
    headers = {'User-Agent':...} # 根据浏览器类型来
    r = requests.get(url,headers=headers,params=parameters)
    print(r.status_code)
```

### 数据解析

返回了一大堆数据，要进行数据的解析，分为HTML和JSON解析

```py

soup = BeautifulSoup(html, 'lxml')
res = soup.findAll('span',{'class':'author'}) #查找条件，得到一个列表

print(res)

output: ['<span class='author'>我叫作者</span>']

soup = BeautifulSoup(html, 'lxml')
res = soup.findAll('span',{'class':'author'}).findChildern(text=True) #查找条件，得到一个列表

print(res)

output: ['我叫作者']


```
#### 数据解析re

正则表达式解析

用re.compile去写一个正则表达式pattern

pattern = re.compile(string)

- re.findall(pattern, string) # 返回列表
- re.match(pattern,string)

```py
if __name__ == '__main__':
    url = 'https://shanghai.8684.cn/so.php'
    parameters = {'q':'同济大学','q1':'东方明珠','k':'p2p'}
    headers = {'User-Agent':'Chrome/85.0.4183.121 Safari/537.36'}
    r = requests.get(url,headers=headers,params=parameters).text
    soup = BeautifulSoup(r,'lxml')
    #res = soup.find_all('div')
    res = soup.findAll('div',{'class':'plan-head'})
    print(res)
```
结果如下
```py
[<div class="plan-head"><span class="plan-no">方案1</span><p>途经8个站点，共换乘1次，步行342米，全程约39分</p></div>, <div class="plan-head"><span class="plan-no">方案2</span><p>途经7个站点，共换乘1次，步行392米，全程约47分</p></div>, <div class="plan-head"><span class="plan-no">方案3</span><p>途经14个站点，共换乘1次，步行154米，全程约1小时18分</p></div>, <div class="plan-head"><span class="plan-no">方案4</span><p>途经7个站点，共换乘1次，步行650米，全程约53分</p></div>, <div class="plan-head"><span class="plan-no">方案5</span><p>途经13个站点，共换乘2次，步行49米，全程约1小时15分</p></div>, <div class="plan-head"><span class="plan-no">方案6</span><p>途经14个站点，共换乘2次，步行1米，全程约1小时20分</p></div>, <div class="plan-head"><span class="plan-no">方案7</span><p>途经13个站点，共换乘2次，步行154米，全程约1小时18分</p></div>, <div class="plan-head"><span class="plan-no">方案8</span><p>途经15个站点，共换乘2次，步行1米，全程约1小时25分</p></div>, <div class="plan-head"><span class="plan-no">方案9</span><p>途经23个站点，共换乘1次，步行169米，全程约2小时03分</p></div>, <div class="plan-head"><span class="plan-no">方案10</span><p>途经11个站点，共换乘1次，步行870米，全程约1小时17分</p></div>]

Process finished with exit code 0
```

### 多重循环采集数据

爬虫是一个多重循环

需要注意为了robust：加入try异常机制
- 注意潜在的错误信息
  - list列表为空
  - 设计随机暂停（time.sleep()）可以搞一个正态分布
    - 模拟正常浏览行为
  - 多准备一些IP地址（代理服务器）

### 通过API获取数据

合法公开接口

API申请：
- 百度开放平台
- 注册
- 控制台应用
- 定义IP限制
- 得到api_key密钥

通过开发文档了解API规则

### 手机APP接口获取

电脑端：Fiddler等
- 捕获HTPPS
- 基本界面
- 设置断点
  - 修改request
  - 修改response
- 过滤会话
- 编码

安卓：
- Wireshark
- Package Capture
- Share for root

### VPS

Virtual Private Server虚拟专用服务器

物理设备上的虚拟机

阿里云、腾讯云

VULTR、DigitalOcean

选择哪个？：分析海底光缆拓扑

使用SSH

windows：Xshell/PuTTY

